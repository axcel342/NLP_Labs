{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axcel342/NLP_Labs/blob/main/Lab3_Language_Representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mea-CemjMt15"
      },
      "source": [
        "# **LAB 3: Language Representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTCiwRXMdlVS"
      },
      "source": [
        "**Language Representation** a.k.a. Text Representation is the process of converting unstructured text data into a structured format (machine-readable form). It involves converting words, phrases, or entire documents into numerical or symbolic representations while preserving meaning and context.\n",
        "\n",
        "It comprise preprocessing the text data followed by selecting a suitable representation scheme, such as Bag-of-Words, TF-IDF etc. to capture the key features and characteristics of the same, in a numerical form that can be processed by machine learning algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUPIYDoTM3yo"
      },
      "source": [
        "# **Objectives:**\n",
        "This lab is designed to introduce students to  fundamental techniques for representing text in a machine-readable format. These techniques form the foundation for various NLP applications, enabling machines to understand and process human language efficiently\n",
        "By the end of this lab, students will be familiar with several key Language Representation tasks which include:\n",
        "\n",
        "1. Text Preprocessing\n",
        "    * Remove Punctuation\n",
        "    * Remove URLs\n",
        "    * Lowercasing\n",
        "    * Tokenization\n",
        "    * Remove Stop Words\n",
        "    * Stemming\n",
        "    * Lemmatization\n",
        "2. Character Encoding\n",
        "    * ASCII\n",
        "    * UTF-8\n",
        "3. Text Representation\n",
        "    * Bag-of-Words (BoW)\n",
        "    * Term Frequency - Inverse Document Frequency (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  **Text Preprocessing**\n",
        "Raw text data is often messy and unstructured, so we need Text Preprocessing, as it cleans and organizes text for better analysis and predictions\n",
        "\n",
        "\n",
        "* Remove Punctuation\n",
        "* Remove URLs\n",
        "* Lowercasing\n",
        "* Tokenization\n",
        "* Remove Stop Words\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "\n"
      ],
      "metadata": {
        "id": "ogTyWzhdWAdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove Punctuation**"
      ],
      "metadata": {
        "id": "7Z_70YsO2c3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello, world! How's it going?\"\n",
        "text_no_punct = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(text_no_punct)  # Output: Hello world Hows it going"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXJT-gNIY2Nt",
        "outputId": "44d5f9f2-bc75-4aaa-b9ee-a56793e6934f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world Hows it going\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove URLs**"
      ],
      "metadata": {
        "id": "uySpPCrt2hUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Check this out: https://example.com for more details.\"\n",
        "text_no_urls = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "print(text_no_urls)  # Output: Check this out:  for more details."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbiZcvsla4N0",
        "outputId": "73485c24-87f9-4e25-a88c-db57bcf2a445"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check this out:  for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lowercasing**"
      ],
      "metadata": {
        "id": "n-J0Xp6l2yeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"HELLO World! This is an Example.\"\n",
        "text_lower = text.lower()\n",
        "print(text_lower)  # Output: hello world! this is an example."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOVaPSq1biHL",
        "outputId": "36f3597c-3850-4c8a-e60c-64808aeffae2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world! this is an example.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**"
      ],
      "metadata": {
        "id": "8SuBpf0D6i73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19T3pnaJ6pX7",
        "outputId": "e80872be-d50b-4069-e1a0-e80b6e2125fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is an example sentence. to demonstrate stop word removal.\"\n",
        "words = word_tokenize(text)\n",
        "print(words) # Output: ['this', 'is', 'an', 'example', 'sentence', 'to', 'demonstrate', 'stop', 'word', 'removal', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InXXv3ia8egR",
        "outputId": "5d6c2dbd-57ed-46c4-caf6-721ebd16dc2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'an', 'example', 'sentence', '.', 'to', 'demonstrate', 'stop', 'word', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove Stop Words**"
      ],
      "metadata": {
        "id": "wk0AINHp2rKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the `words` from previous block\n",
        "filtered_text = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "print(filtered_text)  # Output: ['example', 'sentence', 'demonstrate', 'stop', 'word', 'removal', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDR-VcehbfOR",
        "outputId": "9dbccf29-c667-4cf0-b2b5-4f3ee466198c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', '.', 'demonstrate', 'stop', 'word', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**"
      ],
      "metadata": {
        "id": "IciaoQfU28AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "nyUSq5y42TPM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"better\", \"happily\", \"jumping\", \"countries\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)\n",
        "# Output: ['run', 'fli', 'better', 'happili', 'jump', 'countri']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtfw5foQ9xWG",
        "outputId": "b9b6beb5-2455-4992-8d07-38fcfc38f0fd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'better', 'happili', 'jump', 'countri']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatization**"
      ],
      "metadata": {
        "id": "aIB6tNBK2-xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld0X-6qq2Vj0",
        "outputId": "72e63609-1e81-4c6b-919f-fdb289a58587"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"flies\", \"better\", \"happily\", \"jumping\", \"countries\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"n\") for word in words]  # 'n' for noun\n",
        "\n",
        "print(lemmatized_words)  # Output: ['run', 'fly', 'better', 'happily', 'jump']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkDPa4cU9X5A",
        "outputId": "eb50cdd6-790b-457e-a24a-c0e5aeff102e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'fly', 'better', 'happily', 'jumping', 'country']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Valid options for `pos` in `.lemmatize()` are “n” for nouns, “v” for verbs, “a” for adjectives, “r” for adverbs and “s” for satellite adjectives.\n",
        "\n"
      ],
      "metadata": {
        "id": "12h10fYe_w6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task: Perform the Preprocessing Steps sequentially, on the provided example, by following the hints in comments**"
      ],
      "metadata": {
        "id": "B-Jgwzw8m1I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltpQpSmam0uF",
        "outputId": "7884e682-bcc3-4ac1-8959-344452858468"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text to be used for preprocessing\n",
        "text = \"Hello, world! NLP is amazing. Let's learn it at https://example.com.\"\n",
        "\n",
        "# 1. Remove Punctuation from Provided Text\n",
        "text_no_punct = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "# print the output\n",
        "print(\"step 1\",text_no_punct)\n",
        "\n",
        "\n",
        "# 2. Remove URLs from Output of Step 1\n",
        "text_no_urls = re.sub(r'http\\S+|www\\S+', '', text_no_punct)\n",
        "# print the output\n",
        "print(\"step 2\",text_no_urls)\n",
        "\n",
        "\n",
        "# 3. Perform Lowercasing on Output of Step 2\n",
        "text_lower = text_no_urls.lower()\n",
        "# print the output\n",
        "print(\"step 3\",text_lower)\n",
        "\n",
        "\n",
        "# 4. Perform Word and Sentence Tokenization, individually on Output of Step 3\n",
        "words = word_tokenize(text_lower)\n",
        "# print both outputs\n",
        "print(\"step 4 words\",words)\n",
        "\n",
        "sentences = sent_tokenize(text_lower)\n",
        "# print both outputs\n",
        "print(\"step 4 sentences\",sentences)\n",
        "\n",
        "\n",
        "# 5. Remove Stop Words from Output of Step 4 (word tokenize output)\n",
        "filtered_text = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "# print the output\n",
        "print(\"step 5\",filtered_text)\n",
        "\n",
        "\n",
        "# 6. Perform Stemming on Output of Step 5\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_text]\n",
        "\n",
        "# print the output\n",
        "print(\"step 6\",stemmed_words)\n",
        "\n",
        "# 7. Perform Lemmatization on Output of Step 5, making sure POS tag is set for Verb\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in filtered_text]\n",
        "\n",
        "# print the output\n",
        "print(\"step 7\",lemmatized_words)  # Output: ['run', 'fly', 'better', 'happily', 'jump']\n"
      ],
      "metadata": {
        "id": "AkStK_f8-G4y",
        "outputId": "505d014e-2463-403e-a35c-fa1688383163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1 Hello world NLP is amazing Lets learn it at httpsexamplecom\n",
            "step 2 Hello world NLP is amazing Lets learn it at \n",
            "step 3 hello world nlp is amazing lets learn it at \n",
            "step 4 words ['hello', 'world', 'nlp', 'is', 'amazing', 'lets', 'learn', 'it', 'at']\n",
            "step 4 sentences ['hello world nlp is amazing lets learn it at']\n",
            "step 5 ['hello', 'world', 'nlp', 'amazing', 'lets', 'learn']\n",
            "step 6 ['hello', 'world', 'nlp', 'amaz', 'let', 'learn']\n",
            "step 7 ['hello', 'world', 'nlp', 'amaze', 'let', 'learn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  **Character Encoding**\n",
        "Raw text data is often messy and unstructured, so we need Text Preprocessing, as it cleans and organizes text for better analysis and predictions\n",
        "\n",
        "\n",
        "* American Standard Code for Information Interchange (ASCII)\n",
        "* Unicode Transformation Format 8 (UTF-8)"
      ],
      "metadata": {
        "id": "HlyctAVVY3fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **American Standard Code for Information Interchange (ASCII)**\n",
        "\n",
        "ASCII is a character encoding standard that uses binary numbers to represent text, and is used in computers, telecommunications, and other devices."
      ],
      "metadata": {
        "id": "_QwuLFRaEoUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform ASCII encoding and decoding using the `.encode()` and `.decode()` function, where the encoding type is set as `\"ascii\"`.\n",
        "\n",
        "Due to its limited multilingual support, it can not encode Non-ASCII characters, so we need to ignore them!"
      ],
      "metadata": {
        "id": "_zgNGKCnHSmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, دنیا\"\n",
        "\n",
        "# Encode using ASCII (ignoring non-ASCII characters)\n",
        "encoded_text = text.encode(\"ascii\", errors=\"ignore\")\n",
        "print(\"Encoded Text: \", encoded_text)  # Output: b'Caf'\n",
        "\n",
        "# Encode using ASCII (replacing non-ASCII characters)\n",
        "decoded_text = encoded_text.decode(\"ascii\")\n",
        "print(\"Decoded Text: \", decoded_text)  # Output: Hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u79AyCo6xt7u",
        "outputId": "c637b0c8-b9d8-4927-ab48-5036753208f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Text:  b'Hello, '\n",
            "Decoded Text:  Hello, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Unicode Transformation Format 8 (UTF-8)**\n",
        "\n",
        "UTF-8 is a character encoding standard which leverages variable-width encoding, meaning that each character is represented by one to four bytes. It is the most common encoding for the World Wide Web."
      ],
      "metadata": {
        "id": "YzEPaHWhEnY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to ASCII, we can perform UTF-8 encoding and decoding using the `.encode()` and `.decode()` function, where the encoding type is set as `\"UTF-8\"`\n",
        "\n",
        "Due to its vast multilingual support, it can encode Non English characters too!\n",
        "\n"
      ],
      "metadata": {
        "id": "aFzcyuKeIOjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Task: Perform UTF-8 Encoding and Decoding on the provided example, using the hints in comments**"
      ],
      "metadata": {
        "id": "nlpjaPQ9JKfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ہیلو، دنیا\"  # Unicode string in Urdu\n",
        "\n",
        "# Encode to bytes using UTF-8\n",
        "encoded_text = text.encode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# print the output\n",
        "print(\"Encoded Text: \", encoded_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Decode back to string using .decode()\n",
        "decoded_text = encoded_text.decode(\"utf-8\")\n",
        "# print the output\n",
        "print(\"Decoded Text: \", decoded_text)  # Output: Hello\n"
      ],
      "metadata": {
        "id": "tdfclDuUyJ2Z",
        "outputId": "1e9aab0d-01d5-4db7-f0aa-674994784bfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Text:  b'\\xdb\\x81\\xdb\\x8c\\xd9\\x84\\xd9\\x88\\xd8\\x8c \\xd8\\xaf\\xd9\\x86\\xdb\\x8c\\xd8\\xa7'\n",
            "Decoded Text:  ہیلو، دنیا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Text Representation**\n",
        "\n",
        "Text representation is the process of converting unstructured text data into a structured format (machine-readable form) that can be used for natural language processing tasks\n",
        "\n",
        "It involves selecting a suitable representation scheme, such as bag-of-words, TF-IDF, word embeddings, or topic models, to capture the key features and characteristics of the text data in a numerical form that can be processed by machine learning algorithms.\n",
        "\n",
        "\n",
        "a) **Bag-of-Words (BoW) Representation:**\n",
        "\n",
        "It represents text as a vector of word frequencies, ignoring grammar and word order, based on a corpus-wide vocabulary.\n",
        "\n",
        "\n",
        "b) **Term Frequency - Inverse Document Frequency (TF-IDF) Representation:**\n",
        "\n",
        "It is a statistical measure that evaluates a word's importance in a document relative to a collection of documents by combining its frequency in the document (TF) and its rarity across the corpus (IDF).\n",
        "\n",
        "Words that appear frequently across many documents (common words) have lower importance."
      ],
      "metadata": {
        "id": "ygOZDTxNf_tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example:**\n",
        "\n",
        "**Input Text 1:** \"I love NLP.\"\n",
        "\n",
        "**Input Text 2:** \"NLP is good.\"\n",
        "\n",
        "a) **Bag-of-Words (BoW) Representation:**\n",
        "\n",
        "Assuming the above 2 sentences where \"NLP\" is common, while other words are occurring once, the vector assign equal weight to \"NLP\" as the other words.\n",
        "\n",
        "b) **Term Frequency - Inverse Document Frequency (TF-IDF) Representation:**\n",
        "\n",
        "Assuming the above 2 sentences where \"NLP\" is common, while other words are occurring one, the vector assign lower weight to \"NLP\", as compared to other words.\n",
        "\n",
        "Now we will implement BoW and TF-IDF in this lab."
      ],
      "metadata": {
        "id": "i5ChAcYmNczt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BoW**"
      ],
      "metadata": {
        "id": "tgXiBAngRF_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer # for BoW"
      ],
      "metadata": {
        "id": "5yjPtE0WLmwN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input texts\n",
        "text1 = \"I love NLP.\"\n",
        "text2 = \"NLP is an interesting subject.\"\n",
        "\n",
        "# Bag of Words (BoW)\n",
        "\n",
        "# Initialize the CountVectorizer, which converts text into a matrix of token counts\n",
        "bow_vectorizer = CountVectorizer()\n",
        "# Fit and transform the input texts into a BoW matrix\n",
        "bow_matrix = bow_vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "# Feature names and BoW representation\n",
        "print(\"Bag of Words (BoW):\")\n",
        "print(\"Feature Names:\", bow_vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRtvl7UYJyUA",
        "outputId": "88d57420-735f-41bb-f4f2-51d75825f75f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW):\n",
            "Feature Names: ['an' 'interesting' 'is' 'love' 'nlp' 'subject']\n",
            "BoW Matrix:\n",
            " [[0 0 0 1 1 0]\n",
            " [1 1 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: `vectorizer.fit_transform()` build a unique vocabulary by\n",
        "  * Applying Tokenization\n",
        "  * Removing Duplicates\n",
        "  * Lowercasing\n",
        "  * Stop Word Removal"
      ],
      "metadata": {
        "id": "wWeBbQGBOhAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF-IDF**"
      ],
      "metadata": {
        "id": "5rZRySOfQ2ED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Task: Perform TF-IDF on the above example (used in BoW), using the hints in comments**"
      ],
      "metadata": {
        "id": "DS_hXDJvN5V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer # for TF-IDF"
      ],
      "metadata": {
        "id": "o1zb-5XuOwYQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "# Same text to be used as BoW\n",
        "text1 = \"I love NLP.\"\n",
        "text2 = \"NLP is an interesting subject.\"\n",
        "\n",
        "# Initialize the TfidfVectorizer, which transforms text into TF-IDF\n",
        "tdidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the input texts into a TF-IDF matrix\n",
        "tfidf_matrix = tdidf_vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "# Print the Feature names and TF-IDF representation\n",
        "print(\"TF-IDF:\")\n",
        "print(\"Feature Names:\", tdidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "id": "9qR5T6Dszsct",
        "outputId": "c05079fe-3561-49ea-9309-6d4f63a625bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF:\n",
            "Feature Names: ['an' 'interesting' 'is' 'love' 'nlp' 'subject']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.81480247 0.57973867 0.        ]\n",
            " [0.47107781 0.47107781 0.47107781 0.         0.33517574 0.47107781]]\n"
          ]
        }
      ]
    }
  ]
}